<!DOCTYPE html>
<html>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="2.2 数据操作在深度学习中，我们通常会频繁地对数据进行操作。作为动手学深度学习的基础，本节将介绍如何对内存中的数据进行操作。 在PyTorch中，torch.Tensor是存储和变换数据的主要工具。如果你之前用过NumPy，你会发现Tensor和NumPy的多维数组非常类似。然而，Tensor提供GPU计算和自动求梯度等更多功能，这些使Tensor更加适合深度学习。  “tensor”这个单词一">
<meta property="og:type" content="article">
<meta property="og:title" content="inside SteveChao">
<meta property="og:url" content="http://yoursite.com/2019/10/12/PyTorch 数据操作/index.html">
<meta property="og:site_name" content="inside SteveChao">
<meta property="og:description" content="2.2 数据操作在深度学习中，我们通常会频繁地对数据进行操作。作为动手学深度学习的基础，本节将介绍如何对内存中的数据进行操作。 在PyTorch中，torch.Tensor是存储和变换数据的主要工具。如果你之前用过NumPy，你会发现Tensor和NumPy的多维数组非常类似。然而，Tensor提供GPU计算和自动求梯度等更多功能，这些使Tensor更加适合深度学习。  “tensor”这个单词一">
<meta property="og:updated_time" content="2019-10-11T16:07:01.666Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="inside SteveChao">
<meta name="twitter:description" content="2.2 数据操作在深度学习中，我们通常会频繁地对数据进行操作。作为动手学深度学习的基础，本节将介绍如何对内存中的数据进行操作。 在PyTorch中，torch.Tensor是存储和变换数据的主要工具。如果你之前用过NumPy，你会发现Tensor和NumPy的多维数组非常类似。然而，Tensor提供GPU计算和自动求梯度等更多功能，这些使Tensor更加适合深度学习。  “tensor”这个单词一">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.jpg">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>inside SteveChao</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">About</a></li>
         
          <li><a href="/work/">Work</a></li>
         
          <li><a href="/archives/">Articles</a></li>
         
          <li><a href="/deepLearning">Deep Learning</a></li>
         
          <li><a href="/photography/">Photography</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        
        <li><a class="icon" href="/2019/08/13/我和黑夜一起想你/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Post Anterior</span>
      <span id="i-next" class="info" style="display:none;">Post Següent</span>
      <span id="i-top" class="info" style="display:none;">Adalt</span>
      <span id="i-share" class="info" style="display:none;">Compartir Post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2019/10/12/PyTorch 数据操作/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2019/10/12/PyTorch 数据操作/&text="><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2019/10/12/PyTorch 数据操作/&title="><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2019/10/12/PyTorch 数据操作/&is_video=false&description="><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=&body=Check out this article: http://yoursite.com/2019/10/12/PyTorch 数据操作/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2019/10/12/PyTorch 数据操作/&title="><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2019/10/12/PyTorch 数据操作/&title="><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2019/10/12/PyTorch 数据操作/&title="><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2019/10/12/PyTorch 数据操作/&title="><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2019/10/12/PyTorch 数据操作/&name=&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2-2-数据操作"><span class="toc-number">1.</span> <span class="toc-text">2.2 数据操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-1-创建Tensor"><span class="toc-number">1.1.</span> <span class="toc-text">2.2.1 创建Tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#改变形状"><span class="toc-number">1.1.1.</span> <span class="toc-text">改变形状</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-4-运算的内存开销"><span class="toc-number">1.2.</span> <span class="toc-text">2.2.4 运算的内存开销</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-5-Tensor和NumPy相互转换"><span class="toc-number">1.3.</span> <span class="toc-text">2.2.5 Tensor和NumPy相互转换</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Tensor转NumPy"><span class="toc-number">1.3.1.</span> <span class="toc-text">Tensor转NumPy</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-6-Tensor-on-GPU"><span class="toc-number">1.4.</span> <span class="toc-text">2.2.6 Tensor on GPU</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index my4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">inside SteveChao</span>
      </span>
      
    <div class="postdate">
        <time datetime="2019-10-11T16:07:01.653Z" itemprop="datePublished">2019-10-12</time>
    </div>


      

    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="2-2-数据操作"><a href="#2-2-数据操作" class="headerlink" title="2.2 数据操作"></a>2.2 数据操作</h1><p>在深度学习中，我们通常会频繁地对数据进行操作。作为动手学深度学习的基础，本节将介绍如何对内存中的数据进行操作。</p>
<p>在PyTorch中，<code>torch.Tensor</code>是存储和变换数据的主要工具。如果你之前用过NumPy，你会发现<code>Tensor</code>和NumPy的多维数组非常类似。然而，<code>Tensor</code>提供GPU计算和自动求梯度等更多功能，这些使<code>Tensor</code>更加适合深度学习。</p>
<blockquote>
<p>“tensor”这个单词一般可译作“张量”，张量可以看作是一个多维数组。标量可以看作是0维张量，向量可以看作1维张量，矩阵可以看作是二维张量。  </p>
</blockquote>
<h2 id="2-2-1-创建Tensor"><a href="#2-2-1-创建Tensor" class="headerlink" title="2.2.1 创建Tensor"></a>2.2.1 创建<code>Tensor</code></h2><p>我们先介绍<code>Tensor</code>的最基本功能，即<code>Tensor</code>的创建。</p>
<p>首先导入PyTorch：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure></p>
<p>然后我们创建一个5x3的未初始化的<code>Tensor</code>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure></p>
<p>输出：<br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.0000e<span class="string">+00</span>,  1.5846e<span class="string">+29</span>,  0.0000e<span class="string">+00</span>],</span><br><span class="line">        [ 1.5846e<span class="string">+29</span>,  5.6052e<span class="string">-45</span>,  0.0000e<span class="string">+00</span>],</span><br><span class="line">        [ 0.0000e<span class="string">+00</span>,  0.0000e<span class="string">+00</span>,  0.0000e<span class="string">+00</span>],</span><br><span class="line">        [ 0.0000e<span class="string">+00</span>,  0.0000e<span class="string">+00</span>,  0.0000e<span class="string">+00</span>],</span><br><span class="line">        [ 0.0000e<span class="string">+00</span>,  1.5846e<span class="string">+29</span>, <span class="string">-2</span>.4336e<span class="string">+02</span>]])</span><br></pre></td></tr></table></figure></p>
<p>创建一个5x3的随机初始化的<code>Tensor</code>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure></p>
<p>输出：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.4963</span>, <span class="number">0.7682</span>, <span class="number">0.0885</span>],</span><br><span class="line">        [<span class="number">0.1320</span>, <span class="number">0.3074</span>, <span class="number">0.6341</span>],</span><br><span class="line">        [<span class="number">0.4901</span>, <span class="number">0.8964</span>, <span class="number">0.4556</span>],</span><br><span class="line">        [<span class="number">0.6323</span>, <span class="number">0.3489</span>, <span class="number">0.4017</span>],</span><br><span class="line">        [<span class="number">0.0223</span>, <span class="number">0.1689</span>, <span class="number">0.2939</span>]])</span><br></pre></td></tr></table></figure></p>
<p>创建一个5x3的long型全0的<code>Tensor</code>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure></p>
<p>输出：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure></p>
<p>还可以直接根据数据创建:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure></p>
<p>输出：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">tensor</span><span class="params">([<span class="number">5.5000</span>, <span class="number">3.0000</span>])</span></span></span><br></pre></td></tr></table></figure></p>
<p>还可以通过现有的<code>Tensor</code>来创建，此方法会默认重用输入<code>Tensor</code>的一些属性，例如数据类型，除非自定义数据类型。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.float64)  <span class="comment"># 返回的tensor默认具有相同的torch.dtype和torch.device</span></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x = torch.randn_like(x, dtype=torch.float) <span class="comment"># 指定新的数据类型</span></span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure></p>
<p>输出：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], dtype=torch.float64)</span><br><span class="line">tensor([[ <span class="number">0.6035</span>,  <span class="number">0.8110</span>, <span class="number">-0.0451</span>],</span><br><span class="line">        [ <span class="number">0.8797</span>,  <span class="number">1.0482</span>, <span class="number">-0.0445</span>],</span><br><span class="line">        [<span class="number">-0.7229</span>,  <span class="number">2.8663</span>, <span class="number">-0.5655</span>],</span><br><span class="line">        [ <span class="number">0.1604</span>, <span class="number">-0.0254</span>,  <span class="number">1.0739</span>],</span><br><span class="line">        [ <span class="number">2.2628</span>, <span class="number">-0.9175</span>, <span class="number">-0.2251</span>]])</span><br></pre></td></tr></table></figure></p>
<p>我们可以通过<code>shape</code>或者<code>size()</code>来获取<code>Tensor</code>的形状:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(x.size())</span><br><span class="line">print(x.shape)</span><br></pre></td></tr></table></figure></p>
<p>输出：<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([5, 3])</span><br><span class="line">torch.Size([5, 3])</span><br><span class="line">```&gt; 注意：返回的torch.Size其实就是一个tuple, 支持所有tuple的操作。</span><br><span class="line">还有很多函数可以创建`Tensor`，去翻翻官方API就知道了，下表给了一些常用的作参考。</span><br><span class="line"></span><br><span class="line">|<span class="string">函数</span>|<span class="string">功能</span>|</span><br><span class="line">|<span class="string">:---:</span>|<span class="string">:---:</span>|</span><br><span class="line">|<span class="string">Tensor(*sizes)</span>|<span class="string">基础构造函数</span>|</span><br><span class="line">|<span class="string">tensor(data,)</span>|<span class="string">类似np.array的构造函数</span>|</span><br><span class="line">|<span class="string">ones(*sizes)</span>|<span class="string">全1Tensor</span>|</span><br><span class="line">|<span class="string">zeros(*sizes)</span>|<span class="string">全0Tensor</span>|</span><br><span class="line">|<span class="string">eye(*sizes)</span>|<span class="string">对角线为1，其他为0</span>|</span><br><span class="line">|<span class="string">arange(s,e,step</span>|<span class="string">从s到e，步长为step</span>|</span><br><span class="line">|<span class="string">linspace(s,e,steps)</span>|<span class="string">从s到e，均匀切分成steps份</span>|</span><br><span class="line">|<span class="string">rand_randn(*sizes)</span>|<span class="string">均匀_标准分布</span>|</span><br><span class="line">|<span class="string">normal(mean,std)_uniform(from,to)</span>|<span class="string">正态分布_均匀分布</span>|</span><br><span class="line">|<span class="string">randperm(m)</span>|<span class="string">随机排列</span>|</span><br><span class="line"></span><br><span class="line">这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu)。</span><br><span class="line"></span><br><span class="line"><span class="comment">## 2.2.2 操作</span></span><br><span class="line">本小节介绍`Tensor`的各种操作。</span><br><span class="line"><span class="comment">### 算术操作</span></span><br><span class="line">在PyTorch中，同一种操作可能有很多种形式，下面用加法作为例子。</span><br><span class="line"><span class="symbol">*</span> <span class="symbol">*</span><span class="symbol">*</span>加法形式一<span class="symbol">*</span><span class="symbol">*</span>```python</span><br><span class="line">y = torch.rand(5, 3)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure></p>
<ul>
<li><p><em>加法形式二</em><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(torch.add(x, y))</span><br><span class="line">```还可指定输出：```python</span><br><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><em>加法形式三、inplace</em><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># adds x to y</span></span><br><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br><span class="line">```&gt; *注：PyTorch操作inplace版本都有后缀<span class="string">"_"</span>, 例如`x.copy_(y), x.t_()`*</span><br><span class="line"></span><br><span class="line">以上几种形式的输出均为：</span><br></pre></td></tr></table></figure></p>
</li>
</ul>
<p>tensor([[ 1.3967,  1.0892,  0.4369],<br>        [ 1.6995,  2.0453,  0.6539],<br>        [-0.1553,  3.7016, -0.3599],<br>        [ 0.7536,  0.0870,  1.2274],<br>        [ 2.5046, -0.1913,  0.4760]])<br><figure class="highlight plain"><figcaption><span>索引</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">我们还可以使用类似NumPy的索引操作来访问`Tensor`的一部分，需要注意的是：**索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。**</span><br><span class="line">```python</span><br><span class="line">y = x[0, :]</span><br><span class="line">y += 1</span><br><span class="line">print(y)</span><br><span class="line">print(x[0, :]) # 源tensor也被改了</span><br></pre></td></tr></table></figure></p>
<p>输出：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">1.6035</span>, <span class="number">1.8110</span>, <span class="number">0.9549</span>])</span><br><span class="line">tensor([<span class="number">1.6035</span>, <span class="number">1.8110</span>, <span class="number">0.9549</span>])</span><br></pre></td></tr></table></figure></p>
<p>除了常用的索引选择数据之外，PyTorch还提供了一些高级的选择函数:</p>
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">index_select(input, dim, index)</td>
<td style="text-align:center">在指定维度dim上选取，比如选取某些行、某些列</td>
</tr>
<tr>
<td style="text-align:center">masked_select(input, mask)</td>
<td style="text-align:center">例子如上，a[a&gt;0]，使用ByteTensor进行选取</td>
</tr>
<tr>
<td style="text-align:center">non_zero(input)</td>
<td style="text-align:center">非0元素的下标</td>
</tr>
<tr>
<td style="text-align:center">gather(input, dim, index)</td>
<td style="text-align:center">根据index，在dim维度上选取数据，输出的size与index一样</td>
</tr>
</tbody>
</table>
<p>这里不详细介绍，用到了再查官方文档。</p>
<h3 id="改变形状"><a href="#改变形状" class="headerlink" title="改变形状"></a>改变形状</h3><p>用<code>view()</code>来改变<code>Tensor</code>的形状：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y = x.view(<span class="number">15</span>)</span><br><span class="line">z = x.view(<span class="number">-1</span>, <span class="number">5</span>)  <span class="comment"># -1所指的维度可以根据其他维度的值推出来</span></span><br><span class="line">print(x.size(), y.size(), z.size())</span><br></pre></td></tr></table></figure></p>
<p>输出：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">torch</span><span class="selector-class">.Size</span>(<span class="selector-attr">[5, 3]</span>) <span class="selector-tag">torch</span><span class="selector-class">.Size</span>(<span class="selector-attr">[15]</span>) <span class="selector-tag">torch</span><span class="selector-class">.Size</span>(<span class="selector-attr">[3, 5]</span>)</span><br></pre></td></tr></table></figure></p>
<p><em>注意<code>view()</code>返回的新tensor与源tensor共享内存（其实是同一个tensor），也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察角度)</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x += <span class="number">1</span></span><br><span class="line">print(x)</span><br><span class="line">print(y) <span class="comment"># 也加了1</span></span><br></pre></td></tr></table></figure></p>
<p>输出：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.6035</span>, <span class="number">1.8110</span>, <span class="number">0.9549</span>],</span><br><span class="line">        [<span class="number">1.8797</span>, <span class="number">2.0482</span>, <span class="number">0.9555</span>],</span><br><span class="line">        [<span class="number">0.2771</span>, <span class="number">3.8663</span>, <span class="number">0.4345</span>],</span><br><span class="line">        [<span class="number">1.1604</span>, <span class="number">0.9746</span>, <span class="number">2.0739</span>],</span><br><span class="line">        [<span class="number">3.2628</span>, <span class="number">0.0825</span>, <span class="number">0.7749</span>]])</span><br><span class="line">tensor([<span class="number">1.6035</span>, <span class="number">1.8110</span>, <span class="number">0.9549</span>, <span class="number">1.8797</span>, <span class="number">2.0482</span>, <span class="number">0.9555</span>, <span class="number">0.2771</span>, <span class="number">3.8663</span>, <span class="number">0.4345</span>,</span><br><span class="line">        <span class="number">1.1604</span>, <span class="number">0.9746</span>, <span class="number">2.0739</span>, <span class="number">3.2628</span>, <span class="number">0.0825</span>, <span class="number">0.7749</span>])</span><br></pre></td></tr></table></figure></p>
<p>所以如果我们想返回一个真正新的副本（即不共享内存）该怎么办呢？Pytorch还提供了一个<code>reshape()</code>可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用<code>clone</code>创造一个副本然后再使用<code>view</code>。<a href="https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch" target="_blank" rel="noopener">参考此处</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_cp = x.clone().view(<span class="number">15</span>)</span><br><span class="line">x -= <span class="number">1</span></span><br><span class="line">print(x)</span><br><span class="line">print(x_cp)</span><br></pre></td></tr></table></figure></p>
<p>输出:<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">0.6035</span>,  <span class="number">0.8110</span>, <span class="number">-0.0451</span>],</span><br><span class="line">        [ <span class="number">0.8797</span>,  <span class="number">1.0482</span>, <span class="number">-0.0445</span>],</span><br><span class="line">        [<span class="number">-0.7229</span>,  <span class="number">2.8663</span>, <span class="number">-0.5655</span>],</span><br><span class="line">        [ <span class="number">0.1604</span>, <span class="number">-0.0254</span>,  <span class="number">1.0739</span>],</span><br><span class="line">        [ <span class="number">2.2628</span>, <span class="number">-0.9175</span>, <span class="number">-0.2251</span>]])</span><br><span class="line">tensor([<span class="number">1.6035</span>, <span class="number">1.8110</span>, <span class="number">0.9549</span>, <span class="number">1.8797</span>, <span class="number">2.0482</span>, <span class="number">0.9555</span>, <span class="number">0.2771</span>, <span class="number">3.8663</span>, <span class="number">0.4345</span>,</span><br><span class="line">        <span class="number">1.1604</span>, <span class="number">0.9746</span>, <span class="number">2.0739</span>, <span class="number">3.2628</span>, <span class="number">0.0825</span>, <span class="number">0.7749</span>])</span><br><span class="line">```&gt; 使用`clone`还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源`Tensor`。</span><br><span class="line">另外一个常用的函数就是`item()`, 它可以将一个标量`Tensor`转换成一个Python number：</span><br><span class="line">```python</span><br><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br></pre></td></tr></table></figure></p>
<p>输出：<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">tensor([2.3466])</span><br><span class="line">2.3466382026672363</span><br><span class="line">```<span class="comment">### 线性代数</span></span><br><span class="line">另外，PyTorch还支持一些线性函数，这里提一下，免得用起来的时候自己造轮子，具体用法参考官方文档。如下表所示：</span><br><span class="line"></span><br><span class="line">|<span class="string"> 函数	</span>|<span class="string">功能</span>|</span><br><span class="line">|<span class="string">:---:</span>|<span class="string">:---:</span>|</span><br><span class="line">|<span class="string">trace</span>|<span class="string">	对角线元素之和(矩阵的迹)</span>|</span><br><span class="line">|<span class="string">diag</span>|<span class="string">	对角线元素</span>|</span><br><span class="line">|<span class="string">triu_tril	</span>|<span class="string">矩阵的上三角_下三角，可指定偏移量</span>|</span><br><span class="line">|<span class="string">mm/bmm	</span>|<span class="string">矩阵乘法，batch的矩阵乘法</span>|</span><br><span class="line">|<span class="string">addmm_addbmm_addmv_addr_badbmm..</span>|<span class="string">	矩阵运算</span>|</span><br><span class="line">|<span class="string">t</span>|<span class="string">转置</span>|</span><br><span class="line">|<span class="string">dot_cross</span>|<span class="string">	内积_外积</span>|</span><br><span class="line">|<span class="string">inverse	</span>|<span class="string">求逆矩阵</span>|</span><br><span class="line">|<span class="string">svd	</span>|<span class="string">奇异值分解</span>|</span><br><span class="line"></span><br><span class="line">PyTorch中的`Tensor`支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，可参考[官方文档](https://pytorch.org/docs/stable/tensors.html)。</span><br><span class="line"></span><br><span class="line"><span class="comment">## 2.2.3 广播机制</span></span><br><span class="line">前面我们看到如何对两个形状相同的`Tensor`做按元素运算。当对两个形状不同的`Tensor`按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个`Tensor`形状相同后再按元素运算。例如：</span><br><span class="line">```python</span><br><span class="line">x = torch.arange(1, 3).view(1, 2)</span><br><span class="line">print(x)</span><br><span class="line">y = torch.arange(1, 4).view(3, 1)</span><br><span class="line">print(y)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure></p>
<p>输出：<br><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="string">[[1, 2]]</span>)</span><br><span class="line">tensor(<span class="string">[[1],</span></span><br><span class="line"><span class="string">        [2],</span></span><br><span class="line"><span class="string">        [3]]</span>)</span><br><span class="line">tensor(<span class="string">[[2, 3],</span></span><br><span class="line"><span class="string">        [3, 4],</span></span><br><span class="line"><span class="string">        [4, 5]]</span>)</span><br></pre></td></tr></table></figure></p>
<p>由于<code>x</code>和<code>y</code>分别是1行2列和3行1列的矩阵，如果要计算<code>x + y</code>，那么<code>x</code>中第一行的2个元素被广播（复制）到了第二行和第三行，而<code>y</code>中第一列的3个元素被广播（复制）到了第二列。如此，就可以对2个3行2列的矩阵按元素相加。</p>
<h2 id="2-2-4-运算的内存开销"><a href="#2-2-4-运算的内存开销" class="headerlink" title="2.2.4 运算的内存开销"></a>2.2.4 运算的内存开销</h2><p>前面说了，索引、<code>view</code>是不会开辟新内存的，而像<code>y = x + y</code>这样的运算是会新开内存的，然后将<code>y</code>指向新内存。为了演示这一点，我们可以使用Python自带的<code>id</code>函数：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y = torch.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">id_before = id(y)</span><br><span class="line">y = y + x</span><br><span class="line">print(id(y) == id_before) <span class="comment"># False</span></span><br></pre></td></tr></table></figure>
<p>如果想指定结果到原来的<code>y</code>的内存，我们可以使用前面介绍的索引来进行替换操作。在下面的例子中，我们把<code>x + y</code>的结果通过<code>[:]</code>写进<code>y</code>对应的内存中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y = torch.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">id_before = id(y)</span><br><span class="line">y[:] = y + x</span><br><span class="line">print(id(y) == id_before) <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<p>我们还可以使用运算符全名函数中的<code>out</code>参数或者自加运算符<code>+=</code>(也即<code>add_()</code>)达到上述效果，例如<code>torch.add(x, y, out=y)</code>和<code>y += x</code>(<code>y.add_(x)</code>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y = torch.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">id_before = id(y)</span><br><span class="line">torch.add(x, y, out=y) <span class="comment"># y += x, y.add_(x)</span></span><br><span class="line">print(id(y) == id_before) <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<h2 id="2-2-5-Tensor和NumPy相互转换"><a href="#2-2-5-Tensor和NumPy相互转换" class="headerlink" title="2.2.5 Tensor和NumPy相互转换"></a>2.2.5 <code>Tensor</code>和NumPy相互转换</h2><p>我们很容易用<code>numpy()</code>和<code>from_numpy()</code>将<code>Tensor</code>和NumPy中的数组相互转换。但是需要注意的一点是：<br><em>这两个函数所产生的的<code>Tensor</code>和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！</em></p>
<blockquote>
<p>还有一个常用的将NumPy中的array转换成<code>Tensor</code>的方法就是<code>torch.tensor()</code>, 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的<code>Tensor</code>和原来的数据不再共享内存。  </p>
</blockquote>
<h3 id="Tensor转NumPy"><a href="#Tensor转NumPy" class="headerlink" title="Tensor转NumPy"></a><code>Tensor</code>转NumPy</h3><p>使用<code>numpy()</code>将<code>Tensor</code>转换成NumPy数组:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">b = a.numpy()</span><br><span class="line">print(a, b)</span><br><span class="line"></span><br><span class="line">a += <span class="number">1</span></span><br><span class="line">print(a, b)</span><br><span class="line">b += <span class="number">1</span></span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure></p>
<p>输出：<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]) [<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line">tensor([<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>]) [<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]</span><br><span class="line">tensor([<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>]) [<span class="number">3.</span> <span class="number">3.</span> <span class="number">3.</span> <span class="number">3.</span> <span class="number">3.</span>]</span><br><span class="line">```### NumPy数组转`Tensor`</span><br><span class="line">使用`from_numpy()`将NumPy数组转换成`Tensor`:</span><br><span class="line">```python</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">print(a, b)</span><br><span class="line"></span><br><span class="line">a += <span class="number">1</span></span><br><span class="line">print(a, b)</span><br><span class="line">b += <span class="number">1</span></span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure></p>
<p>输出：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>] tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>], dtype=torch.float64)</span><br><span class="line">[<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>] tensor([<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>], dtype=torch.float64)</span><br><span class="line">[<span class="number">3.</span> <span class="number">3.</span> <span class="number">3.</span> <span class="number">3.</span> <span class="number">3.</span>] tensor([<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>], dtype=torch.float64)</span><br></pre></td></tr></table></figure></p>
<p>所有在CPU上的<code>Tensor</code>（除了<code>CharTensor</code>）都支持与NumPy数组相互转换。</p>
<p>此外上面提到还有一个常用的方法就是直接用<code>torch.tensor()</code>将NumPy数组转换成<code>Tensor</code>，需要注意的是该方法总是会进行数据拷贝，返回的<code>Tensor</code>和原来的数据不再共享内存。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c = torch.tensor(a)</span><br><span class="line">a += <span class="number">1</span></span><br><span class="line">print(a, c)</span><br></pre></td></tr></table></figure></p>
<p>输出<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">4.</span> <span class="number">4.</span> <span class="number">4.</span> <span class="number">4.</span> <span class="number">4.</span>] tensor([<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>], dtype=torch.float64)</span><br></pre></td></tr></table></figure></p>
<h2 id="2-2-6-Tensor-on-GPU"><a href="#2-2-6-Tensor-on-GPU" class="headerlink" title="2.2.6 Tensor on GPU"></a>2.2.6 <code>Tensor</code> on GPU</h2><p>用方法<code>to()</code>可以将<code>Tensor</code>在CPU和GPU（需要硬件支持）之间相互移动。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下代码只有在PyTorch GPU版本上才会执行</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)          <span class="comment"># GPU</span></span><br><span class="line">    y = torch.ones_like(x, device=device)  <span class="comment"># 直接创建一个在GPU上的Tensor</span></span><br><span class="line">    x = x.to(device)                       <span class="comment"># 等价于 .to("cuda")</span></span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))       <span class="comment"># to()还可以同时更改数据类型</span></span><br></pre></td></tr></table></figure></p>
<p>—&gt; 注: 本文主要参考<a href="https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py" target="_blank" rel="noopener">PyTorch官方文档</a>和<a href="https://github.com/chenyuntc/pytorch-book/blob/master/chapter3-Tensor%E5%92%8Cautograd/Tensor.ipynb" target="_blank" rel="noopener">此处</a>，与<a href="https://zh.d2l.ai/chapter_prerequisite/ndarray.html" target="_blank" rel="noopener">原书同一节</a>有很大不同。</p>

  </div>
</article>



    </div>
    
      <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">About</a></li>
         
          <li><a href="/work/">Work</a></li>
         
          <li><a href="/archives/">Articles</a></li>
         
          <li><a href="/deepLearning">Deep Learning</a></li>
         
          <li><a href="/photography/">Photography</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2-2-数据操作"><span class="toc-number">1.</span> <span class="toc-text">2.2 数据操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-1-创建Tensor"><span class="toc-number">1.1.</span> <span class="toc-text">2.2.1 创建Tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#改变形状"><span class="toc-number">1.1.1.</span> <span class="toc-text">改变形状</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-4-运算的内存开销"><span class="toc-number">1.2.</span> <span class="toc-text">2.2.4 运算的内存开销</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-5-Tensor和NumPy相互转换"><span class="toc-number">1.3.</span> <span class="toc-text">2.2.5 Tensor和NumPy相互转换</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Tensor转NumPy"><span class="toc-number">1.3.1.</span> <span class="toc-text">Tensor转NumPy</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-6-Tensor-on-GPU"><span class="toc-number">1.4.</span> <span class="toc-text">2.2.6 Tensor on GPU</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2019/10/12/PyTorch 数据操作/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2019/10/12/PyTorch 数据操作/&text="><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2019/10/12/PyTorch 数据操作/&title="><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2019/10/12/PyTorch 数据操作/&is_video=false&description="><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=&body=Check out this article: http://yoursite.com/2019/10/12/PyTorch 数据操作/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2019/10/12/PyTorch 数据操作/&title="><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2019/10/12/PyTorch 数据操作/&title="><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2019/10/12/PyTorch 数据操作/&title="><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2019/10/12/PyTorch 数据操作/&title="><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2019/10/12/PyTorch 数据操作/&name=&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menú</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Compartir</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Cap amunt</a>
    </div>

  </div>
</div>

    
    <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2019 SteveChao
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">About</a></li>
         
          <li><a href="/work/">Work</a></li>
         
          <li><a href="/archives/">Articles</a></li>
         
          <li><a href="/deepLearning">Deep Learning</a></li>
         
          <li><a href="/photography/">Photography</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

</body>
</html>
<!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

<!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


